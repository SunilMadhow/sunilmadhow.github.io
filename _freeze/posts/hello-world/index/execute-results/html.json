{
  "hash": "94eb125069146e705c68110a0e48a74f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"My New Post\"\ndate: 2025-09-15        # or just delete 'date' while testing\npublished: true         # remove if you had set false anywhere\n---\n\nI recently read this tweet by my friend, Ahmad Beirami. My education in concentration inequalities has been quite utilitarian, and Ahmad's tweet confirmed to me that I am missing out on some deep geometric intuition in statistics. This has led me to revisit some of primitives of statistics with which I have long had merely a pragmatic alliance -- among them the cumulant generating function, the Chernoff bound, and exponential families -- with the hope of forging a firmer friendship. This note represents these efforts. \n\n# The Chernoff Bound, as it was presented to me\n\nHere is how the Chernoff argument typically runs in an undergraduate class. We have an arbitrary random variable, $X \\sim P$, and we wish to bound its tail probability, \n\\[\\mathrm{Pr}[X > t]\\]\n\nI, the teacher, now pull the following rabbit out of my hat: \n$$\\mathrm{Pr}[X > t] = \\inf_\\lambda \\mathrm{Pr}[e^{\\lambda X} > e^{\\lambda t}] \\leq \\inf_\\lambda e^{\\lambda t - \\log \\E[e^{\\lambda X}]]} = e^{-I(t)} $$\n\nWhere $I$ is the rate function (the Legendre dual of the C.G.F). Together, we then crank out pages of calculation to get supertight binomial tail bounds, or possibly characterize subgaussian or subexponential random variables. In the end, I was left with -- at best -- a vague intuition that exponentiation pushes the random variable towards the tightest application of Markov's inequality by making its distribution look more like two point-masses. As a starting point towards making this derivation look less miraculous (and therefore more beautiful), let's simply rewrite the above derivation slightly as a bound on an importance-weighted quantity under the distribution $P_\\lambda$ that satisfies $\\frac{dP_\\lambda}{dP}(x) = e^{\\lambda x - \\log E[e^{\\lambda X}]}$, where we note that the C.G.F. enters the stage merely as a normalizing constant. \n\n$$\\mathrm{Pr}[X > t] = \\E[\\mathbb{1}\\{X > t\\}] = \\E_{P_\\lambda}[\\frac{dP}{dP_\\lambda} \\mathbb{1}\\{X > t\\}]$$\n\nConsider a simple adaptive smoothing idea for a noisy sequence $y_t$: pick a context window $w_t$ that minimizes a local cross-validated loss. Even this toy perspective already suggests **data-dependent** choices can outperform fixed windows.\n\nHere's a tiny Python example comparing fixed vs \"adaptive\" (toy) smoothing on synthetic data:\n\n::: {#a3a073e9 .cell execution_count=1}\n``` {.python .cell-code}\nimport math, random\n\nrandom.seed(0)\nn = 80\nx = list(range(n))\ny = [math.sin(i/8) + 0.3*random.gauss(0,1) for i in x]\n\ndef moving_avg(seq, w):\n    out = []\n    for i in range(len(seq)):\n        L = max(0, i-w)\n        R = min(len(seq), i+w+1)\n        out.append(sum(seq[L:R])/(R-L))\n    return out\n\ndef toy_adaptive(seq):\n    # choose smaller window on \"fast\" changes\n    out = []\n    for i in range(len(seq)):\n        local_var = 0.0\n        for k in range(max(1, i-2), min(len(seq)-1, i+3)):\n            local_var += abs(seq[k+1]-seq[k])\n        w = 1 if local_var/5 > 0.4 else 3\n        L = max(0, i-w); R = min(len(seq), i+w+1)\n        out.append(sum(seq[L:R])/(R-L))\n    return out\n\nfixed = moving_avg(y, 3)\nadapt = toy_adaptive(y)\n\nfixed[:5], adapt[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n([0.12728228688358106,\n  0.1367300035764212,\n  0.2078518806645078,\n  0.2832155700275483,\n  0.3168868723490528],\n [0.12728228688358106,\n  0.1367300035764212,\n  0.2078518806645078,\n  0.2832155700275483,\n  0.3168868723490528])\n```\n:::\n:::\n\n\nAnd a quick identity to keep us honest:\n\n$$\n\\frac{d}{dx} x^2 = 2x.\n$$\n\nMore soon.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}