[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Rates for offline reinforcement learning with adaptively collected data (L4DC 2025). With D. Qiao, M. Yin, and Y.-X. Wang. Preprint: https://arxiv.org/abs/2306.1406\n\nAKORN: Adaptive knots generated online for regression splines (ICML 2025). With D. Baby and Y.-X. Wang. Code: https://github.com/SunilMadhow/AKORN"
  },
  {
    "objectID": "posts/chernoff/index.html",
    "href": "posts/chernoff/index.html",
    "title": "The Geometry of Prediction, Part I",
    "section": "",
    "text": "I recently read this tweet by my friend, Ahmad Beirami. My approach to concentration inequalities has been quite utilitarian, and Ahmadâ€™s tweet confirmed to me that I am missing out on some deep geometric intuition in statistics. This has led me to revisit some primitives of statistics with which I have long had merely a pragmatic alliance â€“ among them the cumulant generating function, the Chernoff bound, and exponential families â€“ with the hope of forging a firmer friendship.\nThis note will be a preposterously overlong account of the Chernoff bound â€“ one which will hopefully make clear what precisely is happening when we exponentiate before applying Markovâ€™s inequality. Thereâ€™s a cute geometric picture that, while standard, is often not presented in basic courses.\nLong term, I want to explain why the geometry that we discuss here bears on estimation theory and online algortihms. The second case is especially rremarkable, as it shows that we can use statistical tools in entirely adversarial scenarios.\nI donâ€™t wish to get too bogged down in technical details. All the (in)equalities that follow hold under the mild assumption that they hold."
  },
  {
    "objectID": "posts/chernoff/index.html#footnotes",
    "href": "posts/chernoff/index.html#footnotes",
    "title": "The Geometry of Prediction, Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInterestingly, in physics, Boltzmann arrived at exponential tilts after using Stirlingâ€™s formula to relax the multinomial distribution, and Einstein later took them as an ansatz for counting the number of microstates that correspond to a macrostate in a system with interactions (Hugo Touchette )â†©ï¸Ž\nTo be concrete, we can take the set of measures to be those with densities wrt \\(P\\), which we can think about as the sphere in \\(L^1(P)\\). The dual variable then rightly in \\(L^\\infty(P)\\). Then we denote \\(\\langle Q, f\\rangle = \\E_Q[f(X)]\\).â†©ï¸Ž"
  },
  {
    "objectID": "posts/hoeffding/index.html",
    "href": "posts/hoeffding/index.html",
    "title": "A clean proof of Hoeffdingâ€™s inequality",
    "section": "",
    "text": "In the previous post, we made rather a meal out of proving the extremely simple fact that \\Pr[X &gt; t] \\leq e^{- \\sup_\\lambda \\lambda x - \\psi(\\lambda)}\nwhere \\psi(\\lambda) = \\log \\E \\exp{lambda X} is the CGF of X \\sim P.\nIn this post, Iâ€™ll try to make up for it by giving a quick and clean proof of Hoeffdingâ€™s inequality, based on the geometric properties established yesterday.\nIn upper bounding the CGF, Hoeffdingâ€™s inequality â€“ which states that for X \\in [a, b] almost surely, \\psi(\\lambda) \\leq \\E_P[X] + (b - a)^2/4 â€“ allows us to lower bound the rate function, and therefore derive concrete concentration bounds. Letâ€™s just prove this thing.\nThe key point is that, as the log-partition function of the Esscher family of P, \\psi(\\lambda) satisfies the following properties\n\n1 \\psi'(\\lambda) = \\E_{P_{\\lambda}}[X]\n2 \\psi''(\\lambda) = \\Var_{P_{\\lambda}}[X].\n\nTaylor expanding \\psi around 0 gives\n\\psi(\\lambda) = \\psi(0) + \\psi'(0) + \\psi''(\\zeta)/2 \\ \\ \\ \\zeta \\in [0, \\lambda]\nso that\n\\psi(\\lambda) = \\E_P[X] + \\Var_{P_{\\lambda}}[X]\nThe RV whose law is P^{\\lambda} is still bounded within [a, b], so we have the uniform bound on its variance (b - a)^2/2 (worst case variance is equal pointmasses on each of a and b). So\n\\psi(\\lambda) = \\E_P[X] + \\frac{(b - a)^2}{4}\nSo the Esscher family being an exponential family made our life super easy. More sophisticated concentration inequalities, like Bernstein and Bennet, have exactly the same template, but we exert finer control over the second moment."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Short research notes and updates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA clean proof of Hoeffdingâ€™s inequality\n\n\n\n\n\n\n\n\nSep 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Geometry of Prediction, Part I\n\n\n\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, Iâ€™m Sunil",
    "section": "",
    "text": "Iâ€™m a PhD student at UC San Diego (HalÄ±cÄ±oÄŸlu Data Science Institute). I work on statistical learning theory and adaptive algorithms for real-world data, with interests in offline reinforcement learning, non-parametric regression, and online learning.\n\nðŸ“¬ Email: smadhow@ucsd.edu\nðŸ”— Links: Google Scholar, GitHub, LinkedIn\nðŸ§ª Research areas: offline RL, non-parametric regression, online learning\n\n\nI am interested in designing theoretically principled algorithms for modern problems in statistics and machine learning. It is important to me that these algorithms be adaptive to the particular problem inst"
  }
]