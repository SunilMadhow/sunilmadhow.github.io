[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Rates for offline reinforcement learning with adaptively collected data (L4DC 2025). With D. Qiao, M. Yin, and Y.-X. Wang. Preprint: https://arxiv.org/abs/2306.1406\n\nAKORN: Adaptive knots generated online for regression splines (ICML 2025). With D. Baby and Y.-X. Wang. Code: https://github.com/SunilMadhow/AKORN"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Short research notes and updates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA clean proof of Hoeffding’s inequality\n\n\n\n\n\n\n\n\nSep 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Geometry of Prediction, Part I\n\n\n\n\n\n\n\n\nSep 15, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hello-world/index.html",
    "href": "posts/hello-world/index.html",
    "title": "Pursuing a flighty rabbit through dual space",
    "section": "",
    "text": "Part 1\nI recently read this tweet by my friend, Ahmad Beirami. My education in concentration inequalities has been quite utilitarian, and Ahmad’s tweet confirmed to me that I am missing out on some deep geometric intuition in statistics. This has led me to revisit some primitives of statistics with which I have long had merely a pragmatic alliance – among them the cumulant generating function, the Chernoff bound, and exponential families – with the hope of forging a firmer friendship. This note represents these efforts.\nLong term, my goal is to see how simple geometric properties of distribution space give rise to estimation theory, large-deviation theory, and online optimziation algorithms.\nI don’t wish to get too bogged down in technical details. All the (in)equalities that follow hold under the mild assumption that they hold.\n\n\nExponential families\nIn their natural form, a parametric density p_\\eta is an exponential family if it has the form p_\\eta(x) = p(x) e^{\\langle \\eta, T(x)\\rangle - A(\\eta)}\nExponential families have many nice properties, including the following\n\n1 \\E_{p_\\eta} [T(x)] = \\nabla A |_\\eta\n2 \\Cov_\\eta(T(x)) = \\nabla^2 A |_\\eta\n3 \\E_\\eta[e^{\\lambda T(x)}] = A(\\eta + \\lambda) - A(\\eta)\n4 Given data x_1, ... x_n \\sim p_\\eta, the MLE for eta, \\hat{\\eta} is satisfies \\nabla_\\eta A(\\hat\\eta) = \\sum_i T(x_i)/n\n5 KL(p_\\eta || p_\\lambda) = D_A(\\lambda || \\eta) where D_A is the Bregman divergence under A.\n\nThese plain lemmas, which can be verified by simple computation, breed copiously amongst themselves to produce theorems that reveal the geometry that underlies so much of classical statistics. We’ll scrutinize their progeny later.\nFor now, however, let us simply notice that, for any base (univariate) density p, we can define an exponential family by multiplying it by e^{\\lambda x} and normalizing appropriately: p_\\lambda(x) = e^{\\lambda x}p(x)/E_p[e^{\\lambda x}] = e^{\\lambda x - \\log \\E [e^{\\lambda X}]}p(x) so that A(\\lambda) = \\psi(\\lambda) = \\log \\E [e^{\\lambda X}] and \\eta = \\lambda. Thus, whenever we linearly tilt a density, the C.G.F. enters the stage merely as a normalizing constant. However, in its capacity as the log-partition function of the so-called Esscher transform , the C.G.F. acquires an interesting role in defining the geometry of \\lambda-space\nIf P is a measure (as opposed to a density), we can define the tilted distribution through the Radon-Nikodym derivative \\frac{dP_\\lambda}{dP} = e^{\\lambda x - \\psi(\\lambda)}. In this note, one can rest assured that essentially nothing is lost by considering this to be a somewhat ostentatious rewrite of the definitional statement p_\\lambda(x)/p(x) = e^{\\lambda x - \\psi(\\lambda)} .\n\n\nThe Chernoff Bound\nHere is how the Chernoff argument typically runs in a Master’s class. We have an arbitrary random variable, X \\sim P, and we wish to bound its tail probability, \\Pr[X &gt; t] \\ ; \\ t &gt; \\E_P[X]\nThe professor now pulls the following rabbit out of his or her hat: \\mathrm{Pr}[X &gt; t] = \\inf_\\lambda \\mathrm{Pr}[e^{\\lambda X} &gt; e^{\\lambda t}] \\leq \\inf_\\lambda e^{- \\lambda t + \\log \\E[e^{\\lambda X}]]} = e^{-I(t)} \nWhere I is the rate function (the Legendre dual of the CGF). Together, we then crank out pages of calculation to get supertight binomial tail bounds, or possibly characterize subgaussian or subexponential random variables. Meanwhile, the rabbit scampers away into a nearby shrub, never to reappear, and leaving me with – at best – a vague intuition that exponentiation pushes the random variable towards the tightest application of Markov’s inequality by making its distribution look more like two point-masses.\nLike penitant students, let’s write this proof three more times and see whether the situation improves.\n\nProof 2: Inscriptions\nAs a starting point towards making this derivation look less miraculous, here’s one rewrite of the above standard proof, using the numerical inequality 1\\{x &gt; t\\} \\leq e^{\\lambda(x - t)} for any \\lambda &gt; 0:\n\\Pr[X &gt; t] = \\E[1\\{X &gt; t}] \\leq \\inf_\\lambda \\E[e^{\\lambda (X - t)}] \\leq \\inf_\\lambda e^{\\psi(\\lambda) - \\lambda t} = e^{-I(t)}\nSo exponentiating and applying Markov’s inequality is the same as inscribing the tail-indicators induced by X in an exponential, and then choosing their rates (\\lambda) to optimize this surrogate on average over X.\n\n\nProof 3: Distributionspace\nIf numerical inequalities feel stringy to you, we can think of this in the space of distributions, equipped with the KL divergence. Recall that P is our law for X, and define E = \\{X &gt; t\\}. Let’s also define Q = P(\\cdot | E), because we then have KL(Q || P) = \\log P(E) and thus for all \\lambda &gt; 0\n0 \\leq KL(Q || P_{\\lambda}) = \\E_Q [\\log \\frac{dQ}{dP}\\frac {dP}{dP_{\\lambda}}] = -\\log P(E) - \\lambda \\E_Q[x] + \\psi(\\lambda) \\leq -\\log P(E) - \\lambda t + \\psi(\\lambda)\nsince Q conditions on X &gt; t.\nSo exponentiating and applying Markov’s inequality is the same as bounding e^{KL(P(\\cdot | E) || P)} \\leq e^{KL(Q || P_{\\lambda}) + \\E_P [\\frac{dP_{\\lambda}}{dP} | E]} \\leq e^{\\E_P [\\frac{dP_{\\lambda}}{dP} | E]} \nand then continuing along our previous argument e^{\\E_P [\\frac{dP_{\\lambda}}{dP} | E]} \\leq ...\n\n\nProof 四: Tilting\nNow, we will write the tail as an importance-weighted expectation under the distribution P_\\lambda that satisfies \\frac{dP_\\lambda}{dP}(x) = e^{\\lambda x - \\log E[e^{\\lambda X}]}. For any \\lambda &gt; 0, we have\n\\mathrm{Pr}[X &gt; t] = \\E[\\mathbb{1}\\{X &gt; t\\}] = \\E_{P_\\lambda}[\\frac{dP}{dP_\\lambda} \\mathbb{1}\\{X &gt; t\\}] \\leq e^{- \\lambda t + \\psi(\\lambda)}\\Pr_{P_\\lambda}[X &gt; t]\nWe really have not done much here – we’ve just made the call to Markov’s inequality more explicit. But doing this reminds us that what we’re doing when we use Markov’s inequality is bounding the probability of the upper tail by 1. And so what we’re doing when we apply the Chernoff method is linearly tilting the base measure P according to some slope \\lambda and bounding the tilted tail by 1. In doing this, we incur an additional term, e^{-\\lambda t + \\psi(\\lambda)}, which is optimized by setting the derivative to zero: \\psi'(\\lambda^*) = t. By the properties of exponential families, this actually tells us that, at the optimal tilt, \\lambda^* \\E_{P_{\\lambda^*}}[X] = \\psi'(\\lambda^*) = t That is, \\lambda^* is the (unique) tilt that fixes the tilted mean to be t!\nWhat is e^{-\\lambda^* t + \\psi(\\lambda^*)} ? Well KL(P_{\\lambda^*} || P) = \\lambda \\E_{P_{\\lambda^*}}[X] - \\psi(\\lambda) = I(a)\nThis is a little better. The Chernoff bound is going to linearly tilt P until the tilted mean is t, and then pay for doing so with e^{-I(t)} = e^{-D(P_{\\lambda^*} || P)}. So the more we have to linearly tilt P have the event t be unsurprising, the tighter our tail bound is.\nSaid another way, we take the MLE estimate for \\lambda among the family of linear tilts of P under the observation x = t, and see how far it is from P in the Bregman geometry induced by the C.G.F potential (i.e. in the Fisher-Information metric). This can be seen Properties 4 and 5 of Exponential Families above.\n\n\\Pr[X &gt; t] = \\E_{P_{\\lambda^*}}[e^{\\lambda X - \\psi (\\lambda)}\\ind \\{X &gt; t\\}]\n\n\n\nUnderlying Geometry\nWe just saw that the Chernoff bound can be written as \\Pr[X &gt; t] \\leq e^{KL(P_{\\lambda^*} || P)}, with \\lambda^* chosen to make \\E_{P_{\\lambda^*}}[X] = t. In this part, we will prove that KL(P_{\\lambda^*} || P) = \\min_{Q \\ : \\ \\E_Q[X] = t}KL(Q || P). This lets us write the Chernoff bound as\n\\Pr[X &gt; t] \\leq e^{-\\min_{Q \\ : \\ \\E_Q[X] = t}KL(Q || P)}\nwhich has the pithy english translation: “the tail of X is controled by it `distance’ to the set of measures with mean t as measured by KL.” Showing this is easy, since all we need to do is to show that P_{\\lambda^*} is optimal on the RHS. But in doing so, we will underst\n\\inf_{Q \\ : \\ \\E_Q[X] = t}KL(Q || P) is a dual formulation of \\inf_{\\lambda} \\psi(\\lambda) - \\lambda t, where, as is common, the hard constraint leaps gracefully into the objective as a penalty.\n\n\nMaximum Likelihood\n\n\nPart 2: Online Learning and AdaBoost\nHere is yet another view of the Chernoff bound. We can inscribe a step function in an exponential to get \\Pr[X &gt; t] = \\E[1\\{X &gt; t\\}] \\leq \\E[e^{\\lambda (X - t)}] \\leq e^{\\psi(\\lambda) - \\lambda x}\nIn measure space, under the map\n\n\nExponential families (reprise)\nThe KL divergence, ever indifferent to the axions of distannces, fails to satisfy the triangle inequality.\n\n\nSanov’s theorem.\n\n\nPart 3: Hedge is a Chernoff bound\nIs exactly optimizing the Chernoff upper bound, using losses as sufficient statistics. Hedge is nonstatistical MLE"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Sunil",
    "section": "",
    "text": "I’m a PhD student at UC San Diego (Halıcıoğlu Data Science Institute). I work on statistical learning theory and adaptive algorithms for real-world data, with interests in offline reinforcement learning, non-parametric regression, and online learning.\n\n📬 Email: smadhow@ucsd.edu\n🔗 Links: Google Scholar, GitHub, LinkedIn\n🧪 Research areas: offline RL, non-parametric regression, online learning\n\n\nI am interested in designing theoretically principled algorithms for modern problems in statistics and machine learning. It is important to me that these algorithms be adaptive to the particular problem inst"
  },
  {
    "objectID": "posts/hello-world/index.html#proof-四-tilting",
    "href": "posts/hello-world/index.html#proof-四-tilting",
    "title": "Pursuing a flighty rabbit through dual space",
    "section": "Proof 四: Tilting",
    "text": "Proof 四: Tilting\nNow, we will write the tail as an importance-weighted expectation under the distribution P_\\lambda that satisfies \\frac{dP_\\lambda}{dP}(x) = e^{\\lambda x - \\log E[e^{\\lambda X}]}. For any \\lambda &gt; 0, we have\n\\mathrm{Pr}[X &gt; t] = \\E[\\mathbb{1}\\{X &gt; t\\}] = \\E_{P_\\lambda}[\\frac{dP}{dP_\\lambda} \\mathbb{1}\\{X &gt; t\\}] \\leq e^{- \\lambda t + \\psi(\\lambda)}\\Pr_{P_\\lambda}[X &gt; t]\nwhere $() := _P [e^{X}]. We really have not done much here – we’ve just made the call to Markov’s inequality more explicit. But doing this reminds us that what we’re doing when we use Markov’s inequality is bounding the probability of the upper tail by 1. And so what we’re doing when we apply the Chernoff method is linearly tilting the base measure P according to some slope \\lambda and bounding the tilted tail by 1. In doing this, we incur an additional term, e^{-\\lambda t + \\psi(\\lambda)}, which is optimized by setting the derivative to zero: \\psi'(\\lambda^*) = t. By the properties of exponential families, this actually tells us that, at the optimal tilt, \\lambda^* \\E_{P_{\\lambda^*}}[X] = \\psi'(\\lambda^*) = t That is, \\lambda^* is the (unique) tilt that fixes the tilted mean to be t!\nWhat is e^{-\\lambda^* t + \\psi(\\lambda^*)} ? Well KL(P_{\\lambda^*} || P) = \\lambda \\E_{P_{\\lambda^*}}[X] - \\psi(\\lambda) = I(a)\nThis is a little better. The Chernoff bound is going to linearly tilt P until the tilted mean is t, and then pay for doing so with e^{-I(t)} = e^{-D(P_{\\lambda^*} || P)}. So the more we have to linearly tilt P have the event t be unsurprising, the tighter our tail bound is.\nSaid another way, we take the MLE estimate for \\lambda among the family of linear tilts of P under the observation x = t, and see how far it is from P in the Bregman geometry induced by the C.G.F potential (i.e. in the Fisher-Information metric). This can be seen Properties 4 and 5 of Exponential Families above.\n\n\\Pr[X &gt; t] = \\E_{P_{\\lambda^*}}[e^{\\lambda X - \\psi (\\lambda)}\\ind \\{X &gt; t\\}]"
  },
  {
    "objectID": "posts/hello-world copy/index.html",
    "href": "posts/hello-world copy/index.html",
    "title": "Pursuing a flighty rabbit through dual space",
    "section": "",
    "text": "Part 1\nI recently read this tweet by my friend, Ahmad Beirami. My education in concentration inequalities has been quite utilitarian, and Ahmad’s tweet confirmed to me that I am missing out on some deep geometric intuition in statistics. This has led me to revisit some primitives of statistics with which I have long had merely a pragmatic alliance – among them the cumulant generating function, the Chernoff bound, and exponential families – with the hope of forging a firmer friendship. This note represents these efforts.\nLong term, my goal is to see how simple geometric properties of distribution space give rise to estimation theory, large-deviation theory, and online optimziation algorithms.\nI don’t wish to get too bogged down in technical details. All the (in)equalities that follow hold under the mild assumption that they hold.\n\n\nExponential families\nIn their natural form, a parametric density p_\\eta is an exponential family if it has the form p_\\eta(x) = p(x) e^{\\langle \\eta, T(x)\\rangle - A(\\eta)}\nExponential families have many nice properties, including the following\n\n1 \\E_{p_\\eta} [T(x)] = \\nabla A |_\\eta\n2 \\Cov_\\eta(T(x)) = \\nabla^2 A |_\\eta\n3 \\E_\\eta[e^{\\lambda T(x)}] = A(\\eta + \\lambda) - A(\\eta)\n4 Given data x_1, ... x_n \\sim p_\\eta, the MLE for eta, \\hat{\\eta} is satisfies \\nabla_\\eta A(\\hat\\eta) = \\sum_i T(x_i)/n\n5 KL(p_\\eta || p_\\lambda) = D_A(\\lambda || \\eta) where D_A is the Bregman divergence under A.\n\nThese plain lemmas, which can be verified by simple computation, breed copiously amongst themselves to produce theorems that reveal the geometry that underlies so much of classical statistics. We’ll scrutinize their progeny later.\nFor now, however, let us simply notice that, for any base (univariate) density p, we can define an exponential family by multiplying it by e^{\\lambda x} and normalizing appropriately: p_\\lambda(x) = e^{\\lambda x}p(x)/E_p[e^{\\lambda x}] = e^{\\lambda x - \\log \\E [e^{\\lambda X}]}p(x) so that A(\\lambda) = \\psi(\\lambda) = \\log \\E [e^{\\lambda X}] and \\eta = \\lambda. Thus, whenever we linearly tilt a density, the C.G.F. enters the stage merely as a normalizing constant. However, in its capacity as the log-partition function of the so-called Esscher transform , the C.G.F. acquires an interesting role in defining the geometry of \\lambda-space\nIf P is a measure (as opposed to a density), we can define the tilted distribution through the Radon-Nikodym derivative \\frac{dP_\\lambda}{dP} = e^{\\lambda x - \\psi(\\lambda)}. In this note, one can rest assured that essentially nothing is lost by considering this to be a somewhat ostentatious rewrite of the definitional statement p_\\lambda(x)/p(x) = e^{\\lambda x - \\psi(\\lambda)} .\n\n\nThe Chernoff Bound\nHere is how the Chernoff argument typically runs in a Master’s class. We have an arbitrary random variable, X \\sim P, and we wish to bound its tail probability, \\Pr[X &gt; t] \\ ; \\ t &gt; \\E_P[X]\nThe professor now pulls the following rabbit out of his or her hat: \\mathrm{Pr}[X &gt; t] = \\inf_\\lambda \\mathrm{Pr}[e^{\\lambda X} &gt; e^{\\lambda t}] \\leq \\inf_\\lambda e^{- \\lambda t + \\log \\E[e^{\\lambda X}]]} = e^{-I(t)} \nWhere I is the rate function (the Legendre dual of the CGF). Together, we then crank out pages of calculation to get supertight binomial tail bounds, or possibly characterize subgaussian or subexponential random variables. Meanwhile, the rabbit scampers away into a nearby shrub, never to reappear, and leaving me with – at best – a vague intuition that exponentiation pushes the random variable towards the tightest application of Markov’s inequality by making its distribution look more like two point-masses.\nLike penitant students, let’s write this proof three more times and see whether the situation improves.\n\nProof 2: Inscriptions\nAs a starting point towards making this derivation look less miraculous, here’s one rewrite of the above standard proof, using the numerical inequality 1\\{x &gt; t\\} \\leq e^{\\lambda(x - t)} for any \\lambda &gt; 0:\n\\Pr[X &gt; t] = \\E[1\\{X &gt; t}] \\leq \\inf_\\lambda \\E[e^{\\lambda (X - t)}] \\leq \\inf_\\lambda e^{\\psi(\\lambda) - \\lambda t} = e^{-I(t)}\nSo exponentiating and applying Markov’s inequality is the same as inscribing the tail-indicators induced by X in an exponential, and then choosing their rates (\\lambda) to optimize this surrogate on average over X.\n\n\nProof 3: Distributionspace\nIf numerical inequalities feel stringy to you, we can think of this in the space of distributions, equipped with the KL divergence. Recall that P is our law for X, and define E = \\{X &gt; t\\}. Let’s also define Q = P(\\cdot | E), because we then have KL(Q || P) = \\log P(E) and thus for all \\lambda &gt; 0\n0 \\leq KL(Q || P_{\\lambda}) = \\E_Q [\\log \\frac{dQ}{dP}\\frac {dP}{dP_{\\lambda}}] = -\\log P(E) - \\lambda \\E_Q[x] + \\psi(\\lambda) \\leq -\\log P(E) - \\lambda t + \\psi(\\lambda)\nsince Q conditions on X &gt; t.\nSo exponentiating and applying Markov’s inequality is the same as bounding e^{KL(P(\\cdot | E) || P)} \\leq e^{KL(Q || P_{\\lambda}) + \\E_P [\\frac{dP_{\\lambda}}{dP} | E]} \\leq e^{\\E_P [\\frac{dP_{\\lambda}}{dP} | E]} \nand then continuing along our previous argument e^{\\E_P [\\frac{dP_{\\lambda}}{dP} | E]} \\leq ...\n\n\nProof 四: Tilting\nNow, we will write the tail as an importance-weighted expectation under the distribution P_\\lambda that satisfies \\frac{dP_\\lambda}{dP}(x) = e^{\\lambda x - \\log E[e^{\\lambda X}]}. For any \\lambda &gt; 0, we have\n\\mathrm{Pr}[X &gt; t] = \\E[\\mathbb{1}\\{X &gt; t\\}] = \\E_{P_\\lambda}[\\frac{dP}{dP_\\lambda} \\mathbb{1}\\{X &gt; t\\}] \\leq e^{- \\lambda t + \\psi(\\lambda)}\\Pr_{P_\\lambda}[X &gt; t]\nWe really have not done much here – we’ve just made the call to Markov’s inequality more explicit. But doing this reminds us that what we’re doing when we use Markov’s inequality is bounding the probability of the upper tail by 1. And so what we’re doing when we apply the Chernoff method is linearly tilting the base measure P according to some slope \\lambda and bounding the tilted tail by 1. In doing this, we incur an additional term, e^{-\\lambda t + \\psi(\\lambda)}, which is optimized by setting the derivative to zero: \\psi'(\\lambda^*) = t. By the properties of exponential families, this actually tells us that, at the optimal tilt, \\lambda^* \\E_{P_{\\lambda^*}}[X] = \\psi'(\\lambda^*) = t That is, \\lambda^* is the (unique) tilt that fixes the tilted mean to be t!\nWhat is e^{-\\lambda^* t + \\psi(\\lambda^*)} ? Well KL(P_{\\lambda^*} || P) = \\lambda \\E_{P_{\\lambda^*}}[X] - \\psi(\\lambda) = I(a)\nThis is a little better. The Chernoff bound is going to linearly tilt P until the tilted mean is t, and then pay for doing so with e^{-I(t)} = e^{-D(P_{\\lambda^*} || P)}. So the more we have to linearly tilt P have the event t be unsurprising, the tighter our tail bound is.\nSaid another way, we take the MLE estimate for \\lambda among the family of linear tilts of P under the observation x = t, and see how far it is from P in the Bregman geometry induced by the C.G.F potential (i.e. in the Fisher-Information metric). This can be seen Properties 4 and 5 of Exponential Families above.\n\n\\Pr[X &gt; t] = \\E_{P_{\\lambda^*}}[e^{\\lambda X - \\psi (\\lambda)}\\ind \\{X &gt; t\\}]\n\n\n\nUnderlying Geometry\nWe just saw that the Chernoff bound can be written as \\Pr[X &gt; t] \\leq e^{KL(P_{\\lambda^*} || P)}, with \\lambda^* chosen to make \\E_{P_{\\lambda^*}}[X] = t. In this part, we will prove that KL(P_{\\lambda^*} || P) = \\min_{Q \\ : \\ \\E_Q[X] = t}KL(Q || P). This lets us write the Chernoff bound as\n\\Pr[X &gt; t] \\leq e^{-\\min_{Q \\ : \\ \\E_Q[X] = t}KL(Q || P)}\nwhich has the pithy english translation: “the tail of X is controled by it `distance’ to the set of measures with mean t as measured by KL.” Showing this is easy, since all we need to do is to show that P_{\\lambda^*} is optimal on the RHS. But in doing so, we will underst\n\\inf_{Q \\ : \\ \\E_Q[X] = t}KL(Q || P) is a dual formulation of \\inf_{\\lambda} \\psi(\\lambda) - \\lambda t, where, as is common, the hard constraint leaps gracefully into the objective as a penalty.\n\n\nMaximum Likelihood\n\n\nPart 2: Online Learning and AdaBoost\nHere is yet another view of the Chernoff bound. We can inscribe a step function in an exponential to get \\Pr[X &gt; t] = \\E[1\\{X &gt; t\\}] \\leq \\E[e^{\\lambda (X - t)}] \\leq e^{\\psi(\\lambda) - \\lambda x}\nIn measure space, under the map\n\n\nExponential families (reprise)\nThe KL divergence, ever indifferent to the axions of distannces, fails to satisfy the triangle inequality.\n\n\nSanov’s theorem.\n\n\nPart 3: Hedge is a Chernoff bound\nIs exactly optimizing the Chernoff upper bound, using losses as sufficient statistics. Hedge is nonstatistical MLE"
  },
  {
    "objectID": "posts/hoeffding/index.html",
    "href": "posts/hoeffding/index.html",
    "title": "A clean proof of Hoeffding’s inequality",
    "section": "",
    "text": "In the previous post, we made rather a meal out of proving the extremely simple fact that \\Pr[X &gt; t] \\leq e^{- \\sup_\\lambda \\lambda x - \\psi(\\lambda)}\nwhere \\psi(\\lambda) = \\log \\E \\exp{lambda X} is the CGF of X \\sim P.\nIn this post, I’ll try to make up for it by giving a quick and clean proof of Hoeffding’s inequality, based on the geometric properties established yesterday.\nIn upper bounding the CGF, Hoeffding’s inequality – which states that for X \\in [a, b] almost surely, \\psi(\\lambda) \\leq \\E_P[X] + (b - a)^2/4 – allows us to lower bound the rate function, and therefore derive concrete concentration bounds. Let’s just prove this thing.\nThe key point is that, as the log-partition function of the Esscher family of P, \\psi(\\lambda) satisfies the following properties\n\n1 \\psi'(\\lambda) = \\E_{P_{\\lambda}}[X]\n2 \\psi''(\\lambda) = \\Var_{P_{\\lambda}}[X].\n\nTaylor expanding \\psi around 0 gives\n\\psi(\\lambda) = \\psi(0) + \\psi'(0) + \\psi''(\\zeta)/2 \\ \\ \\ \\zeta \\in [0, \\lambda]\nso that\n\\psi(\\lambda) = \\E_P[X] + \\Var_{P_{\\lambda}}[X]\nThe RV whose law is P^{\\lambda} is still bounded within [a, b], so we have the uniform bound on its variance (b - a)^2/2 (worst case variance is equal pointmasses on each of a and b). So\n\\psi(\\lambda) = \\E_P[X] + \\frac{(b - a)^2}{4}\nSo the Esscher family being an exponential family made our life super easy. More sophisticated concentration inequalities, like Bernstein and Bennet, have exactly the same template, but we exert finer control over the second moment."
  },
  {
    "objectID": "posts/chernoff/index.html",
    "href": "posts/chernoff/index.html",
    "title": "The Geometry of Prediction, Part I",
    "section": "",
    "text": "I recently read this tweet by my friend, Ahmad Beirami. My approach to concentration inequalities has been quite utilitarian, and Ahmad’s tweet confirmed to me that I am missing out on some deep geometric intuition in statistics. This has led me to revisit some primitives of statistics with which I have long had merely a pragmatic alliance – among them the cumulant generating function, the Chernoff bound, and exponential families – with the hope of forging a firmer friendship.\nThis note will be a preposterously overlong account of the Chernoff bound – one which will hopefully make clear what precisely is happening when we exponentiate before applying Markov’s inequality. There’s a cute geometric picture that, while standard, is often not presented in basic courses.\nLong term, I want to explain why the geometry that we discuss here bears on estimation theory and online algortihms. The second case is especially rremarkable, as it shows that we can use statistical tools in entirely adversarial scenarios.\nI don’t wish to get too bogged down in technical details. All the (in)equalities that follow hold under the mild assumption that they hold."
  },
  {
    "objectID": "posts/chernoff/index.html#footnotes",
    "href": "posts/chernoff/index.html#footnotes",
    "title": "The Geometry of Prediction, Part I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInterestingly, in physics, Boltzmann arrived at exponential tilts after using Stirling’s formula to relax the multinomial distribution, and Einstein later took them as an ansatz for counting the number of microstates that correspond to a macrostate in a system with interactions (Hugo Touchette )↩︎\nTo be concrete, we can take the set of measures to be those with densities wrt \\(P\\), which we can think about as the sphere in \\(L^1(P)\\). The dual variable then rightly in \\(L^\\infty(P)\\). Then we denote \\(\\langle Q, f\\rangle = \\E_Q[f(X)]\\).↩︎"
  },
  {
    "objectID": "posts/chernoff/index.html#geometry",
    "href": "posts/chernoff/index.html#geometry",
    "title": "The Geometry of Prediction, Part I",
    "section": "Geometry",
    "text": "Geometry\n\n\\Pr[X &gt; t] = \\E_{P_{\\lambda^*}}[e^{\\lambda X - \\psi (\\lambda)}\\ind \\{X &gt; t\\}]"
  },
  {
    "objectID": "posts/chernoff/index.html#duality",
    "href": "posts/chernoff/index.html#duality",
    "title": "The Geometry of Prediction, Part I",
    "section": "Duality",
    "text": "Duality\n\n\\Pr[X &gt; t] = \\E_{P_{\\lambda^*}}[e^{\\lambda X - \\psi (\\lambda)}\\ind \\{X &gt; t\\}]"
  }
]